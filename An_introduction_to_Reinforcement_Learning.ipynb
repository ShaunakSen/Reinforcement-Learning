{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "An introduction to Reinforcement Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaunakSen/Reinforcement-Learning/blob/master/An_introduction_to_Reinforcement_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jRtaW1sDLwq",
        "colab_type": "text"
      },
      "source": [
        "## An introduction to Reinforcement Learning\n",
        "\n",
        "[course link](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/#syllabus)\n",
        "\n",
        "[article link](https://www.freecodecamp.org/news/an-introduction-to-reinforcement-learning-4339519de419/)\n",
        "\n",
        "> by Thomas Simonini\n",
        "\n",
        "Reinforcement learning is an important type of Machine Learning where an agent learn how to behave in a environment by performing actions and seeing the results.\n",
        "\n",
        "The idea behind Reinforcement Learning is that an agent will learn from the environment by interacting with it and receiving rewards for performing actions.\n",
        "\n",
        "Learning from interaction with the environment comes from our natural experiences. Imagine you’re a child in a living room. You see a fireplace, and you approach it.\n",
        "\n",
        "It’s warm, it’s positive, you feel good (Positive Reward +1). You understand that fire is a positive thing.\n",
        "\n",
        "But then you try to touch the fire. Ouch! It burns your hand (Negative reward -1). You’ve just understood that fire is positive when you are a sufficient distance away, because it produces warmth. But get too close to it and you will be burned.\n",
        "\n",
        "That’s how humans learn, through interaction. Reinforcement Learning is just a computational approach of learning from action.\n",
        "\n",
        "### The Reinforcement Learning Process\n",
        "\n",
        "\n",
        "![](https://cdn-media-1.freecodecamp.org/images/1*aKYFRoEmmKkybqJOvLt2JQ.png)\n",
        "\n",
        "Let’s imagine an agent learning to play Super Mario Bros as a working example. The Reinforcement Learning (RL) process can be modeled as a loop that works like this:\n",
        "\n",
        "- Our Agent receives state S0 from the Environment (In our case we receive the first frame of our game (state) from Super Mario Bros (environment))\n",
        "- Based on that state S0, agent takes an action A0 (our agent will move right)\n",
        "- Environment transitions to a new state S1 (new frame)\n",
        "- Environment gives some reward R1 to the agent (not dead: +1)\n",
        "\n",
        "\n",
        "This RL loop outputs a sequence of state, action and reward.\n",
        "\n",
        "The goal of the agent is to maximize the expected cumulative reward.\n",
        "\n",
        "### The central idea of the Reward Hypothesis\n",
        "\n",
        "Why is the goal of the agent to maximize the expected cumulative reward?\n",
        "\n",
        "Well, Reinforcement Learning is based on the idea of the reward hypothesis. All goals can be described by the maximization of the expected cumulative reward.\n",
        "\n",
        "That’s why in Reinforcement Learning, to have the best behavior, we need to maximize the expected cumulative reward.\n",
        "\n",
        "The cumulative reward at each time step t can be written as:\n",
        "\n",
        "\n",
        "![](https://cdn-media-1.freecodecamp.org/images/0*ylz4lplMffGQR_g3.)\n",
        "\n",
        "Which is equivalent to:\n",
        "\n",
        "![](https://cdn-media-1.freecodecamp.org/images/1*AFAuM1Y8zmso4yB5mOApZA.png)\n",
        "\n",
        "However, in reality, we can’t just add the rewards like that. The rewards that come sooner (in the beginning of the game) are more probable to happen, since they are more predictable than the long term future reward.\n",
        "\n",
        "![](https://cdn-media-1.freecodecamp.org/images/1*tciNrjN6pW60-h0PiQRiXg.png)\n",
        "\n",
        "Let say your agent is this small mouse and your opponent is the cat. Your goal is to eat the maximum amount of cheese before being eaten by the cat.\n",
        "\n",
        "As we can see in the diagram, it’s more probable to eat the cheese near us than the cheese close to the cat (the closer we are to the cat, the more dangerous it is).\n",
        "\n",
        "As a consequence, the reward near the cat, even if it is bigger (more cheese), **will be discounted. We’re not really sure we’ll be able to eat it.**\n",
        "\n",
        "To discount the rewards, we proceed like this:\n",
        "\n",
        "We define a discount rate called gamma. It must be between 0 and 1.\n",
        "\n",
        "- The larger the gamma, the smaller the discount. This means the learning agent cares more about the long term reward.\n",
        "\n",
        "- On the other hand, the smaller the gamma, the bigger the discount. This means our agent cares more about the short term reward (the nearest cheese).\n",
        "\n",
        "Our discounted cumulative expected rewards is:\n",
        "\n",
        "![](https://cdn-media-1.freecodecamp.org/images/1*zrzRTXt8rtWF5fX__kZ-yQ.png)\n",
        "\n",
        "\n",
        "**To be simple, each reward will be discounted by gamma to the exponent of the time step. As the time step increases, the cat gets closer to us, so the future reward is less and less probable to happen.**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izr5r54oDJY7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}