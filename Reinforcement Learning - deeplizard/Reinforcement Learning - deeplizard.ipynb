{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Series\n",
    "\n",
    "> Based on the tutorial series by DeepLizard: https://www.youtube.com/playlist?list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv\n",
    "\n",
    "---\n",
    "\n",
    "### Markov Decision Processes (MDPs) - Structuring a Reinforcement Learning Problem\n",
    "\n",
    "\n",
    "Markov decision processes give us a way to formalize sequential decision making. In an MDP, we have a decision maker, called an *agent*, that interacts with the *environment* it's placed in. These interactions occur sequentially over time. At each time step, the agent will get some representation of the environment’s *state*. Given this representation, the agent selects an *action* to take. The environment is then transitioned into a new state, and the agent is given a *reward* as a consequence of the previous action.\n",
    "\n",
    "Components of an MDP:\n",
    "- Agent\n",
    "- Environment\n",
    "- State\n",
    "- Action\n",
    "- Reward\n",
    "\n",
    "This process of selecting an action from a given state, transitioning to a new state, and receiving a reward happens sequentially over and over again, which creates something called a *trajectory* that shows the sequence of states, actions, and rewards.\n",
    "\n",
    "Throughout this process, it is the agent’s goal to maximize the total amount of rewards that it receives from taking actions in given states. **This means that the agent wants to maximize not just the immediate reward, but the cumulative rewards it receives over time**.\n",
    "\n",
    "\n",
    "### MDP Notation\n",
    "\n",
    "We’re now going to repeat what we just casually discussed but in a more formal and mathematically notated way.\n",
    "\n",
    "In an MDP, we have a set of states $S$ a set of actions $A$ and a set of rewards $R$. We'll assume that each of these sets has a finite number of elements.\n",
    "\n",
    "At each time step, $t = 0,1,2,\\cdots$ he agent receives some representation of the environment’s state $S_t \\in \\boldsymbol{S}$ Based on this state, the agent selects an action $A_t \\in \\boldsymbol{A}$ his gives us the state-action pair $(S_t,A_t)$\n",
    "\n",
    "\n",
    "Time is then incremented to the next time step $t+1$ and the environment is transitioned to a new state $S_{t+1} \\in \\boldsymbol{S}$ At this time, the agent receives a numerical reward \n",
    "$R_{t+1} \\in \\boldsymbol{R}$ for the action $A_t$ taken from the state $S_t$\n",
    "\n",
    "We can think of the process of receiving a reward as an arbitrary function f that maps state-action pairs to rewards. At each time \n",
    "t, we have\n",
    "\n",
    "$f(S_{t}, A_{t}) = R_{t+1}$\n",
    "\n",
    "The trajectory representing the sequential process of selecting an action from a state, transitioning to a new state, and receiving a reward can be represented as\n",
    "\n",
    "$S_0,A_0,R_1,S_1,A_1,R_2,S_2,A_2,R_3,\\cdots$\n",
    "\n",
    "This diagram nicely illustrates this entire idea.\n",
    "\n",
    "![](https://deeplizard.com/images/MDP-diagram.png)\n",
    "\n",
    "Let's break down this diagram into steps.\n",
    "\n",
    "1. At time t, the environment is in state $S_t$\n",
    "2. The agent observes the current state and selects action $A_t$\n",
    "3. The environment transitions to state $S_{t+1}$ and and grants the agent reward $R_{t+1}$ where $f(S_{t}, A_{t}) = R_{t+1}$\n",
    "4. This process then starts over for the next time step, $t+1$\n",
    "    - Note $t+1$ is no longer in the future, but is now the present. When we cross the dotted line on the bottom left, the diagram shows $t+1$ transforming into the current time step $t$ so that $S_{t+1}$ and $R_{t+1}$ are now $S_t$ and $R_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition Probabilities\n",
    "\n",
    "Since the states $S$ and $R$ are finite the random vars $S_t$ and $R_t$ have well defined probability distributions. In other words, all the possible values that can be assigned to $R_t$ and $S_t$ have some associated probability. **These distributions depend on the preceding state and action that occurred in the previous time step**\n",
    "\n",
    "For example, suppose $s’ \\in \\boldsymbol{S}$ and $r \\in \\boldsymbol{R}$. Then there is some probability that $S_t=s’$ and $R_t=r$, i.e the state and reward will be $s’$ and $r$. This probability is determined by the particular values of the preceding state $s \\in \\boldsymbol{S}$ and action $a \\in \\boldsymbol{A}(s)$.Note that $\\boldsymbol{A}(s)$ is the set of actions that can be taken from state $s$\n",
    "\n",
    "Let’s define this probability.\n",
    "\n",
    "For all $s’ \\in \\boldsymbol{S}$, $s \\in \\boldsymbol{S}$, $r \\in \\boldsymbol{R}$ and $a \\in \\boldsymbol{A}(s)$ we define the probability of the transition to state $s’$ with reward $r$ rom taking action $a$ in state $s$ as :\n",
    "\n",
    "$\\begin{equation*} p\\left( s^{\\prime },r\\mid s,a\\right) =\\Pr \\left\\{ S_{t}=s^{\\prime },R_{t}=r\\mid S_{t-1}=s,A_{t-1}=a\\right\\} \\text{.} \\end{equation*}$\n",
    "\n",
    "\n",
    "The state and reward at time t depend on which of the following?\n",
    "    - State-action pair for time (t-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python37464bit534221b4ba144d7c9105a79e50b044ad"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
