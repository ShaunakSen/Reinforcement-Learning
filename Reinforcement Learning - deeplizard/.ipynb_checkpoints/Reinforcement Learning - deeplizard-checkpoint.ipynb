{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Series\n",
    "\n",
    "> Based on the tutorial series by DeepLizard: https://www.youtube.com/playlist?list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv\n",
    "\n",
    "---\n",
    "\n",
    "### Markov Decision Processes (MDPs) - Structuring a Reinforcement Learning Problem\n",
    "\n",
    "\n",
    "Markov decision processes give us a way to formalize sequential decision making. In an MDP, we have a decision maker, called an *agent*, that interacts with the *environment* it's placed in. These interactions occur sequentially over time. At each time step, the agent will get some representation of the environment’s *state*. Given this representation, the agent selects an *action* to take. The environment is then transitioned into a new state, and the agent is given a *reward* as a consequence of the previous action.\n",
    "\n",
    "Components of an MDP:\n",
    "- Agent\n",
    "- Environment\n",
    "- State\n",
    "- Action\n",
    "- Reward\n",
    "\n",
    "This process of selecting an action from a given state, transitioning to a new state, and receiving a reward happens sequentially over and over again, which creates something called a *trajectory* that shows the sequence of states, actions, and rewards.\n",
    "\n",
    "Throughout this process, it is the agent’s goal to maximize the total amount of rewards that it receives from taking actions in given states. **This means that the agent wants to maximize not just the immediate reward, but the cumulative rewards it receives over time**.\n",
    "\n",
    "\n",
    "### MDP Notation\n",
    "\n",
    "We’re now going to repeat what we just casually discussed but in a more formal and mathematically notated way.\n",
    "\n",
    "In an MDP, we have a set of states $S$ a set of actions $A$ and a set of rewards $R$. We'll assume that each of these sets has a finite number of elements.\n",
    "\n",
    "At each time step, $t = 0,1,2,\\cdots$ he agent receives some representation of the environment’s state $S_t \\in \\boldsymbol{S}$ Based on this state, the agent selects an action $A_t \\in \\boldsymbol{A}$ his gives us the state-action pair $(S_t,A_t)$\n",
    "\n",
    "\n",
    "Time is then incremented to the next time step $t+1$ and the environment is transitioned to a new state $S_{t+1} \\in \\boldsymbol{S}$ At this time, the agent receives a numerical reward \n",
    "$R_{t+1} \\in \\boldsymbol{R}$ for the action $A_t$ taken from the state $S_t$\n",
    "\n",
    "We can think of the process of receiving a reward as an arbitrary function f that maps state-action pairs to rewards. At each time \n",
    "t, we have\n",
    "\n",
    "$f(S_{t}, A_{t}) = R_{t+1}$\n",
    "\n",
    "The trajectory representing the sequential process of selecting an action from a state, transitioning to a new state, and receiving a reward can be represented as\n",
    "\n",
    "$S_0,A_0,R_1,S_1,A_1,R_2,S_2,A_2,R_3,\\cdots$\n",
    "\n",
    "This diagram nicely illustrates this entire idea.\n",
    "\n",
    "![](https://deeplizard.com/images/MDP-diagram.png)\n",
    "\n",
    "Let's break down this diagram into steps.\n",
    "\n",
    "1. At time t, the environment is in state $S_t$\n",
    "2. The agent observes the current state and selects action $A_t$\n",
    "3. The environment transitions to state $S_{t+1}$ and and grants the agent reward $R_{t+1}$ where $f(S_{t}, A_{t}) = R_{t+1}$\n",
    "4. This process then starts over for the next time step, $t+1$\n",
    "    - Note $t+1$ is no longer in the future, but is now the present. When we cross the dotted line on the bottom left, the diagram shows $t+1$ transforming into the current time step $t$ so that $S_{t+1}$ and $R_{t+1}$ are now $S_t$ and $R_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition Probabilities\n",
    "\n",
    "Since the states $S$ and $R$ are finite the random vars $S_t$ and $R_t$ have well defined probability distributions. In other words, all the possible values that can be assigned to $R_t$ and $S_t$ have some associated probability. **These distributions depend on the preceding state and action that occurred in the previous time step**\n",
    "\n",
    "For example, suppose $s’ \\in \\boldsymbol{S}$ and $r \\in \\boldsymbol{R}$. Then there is some probability that $S_t=s’$ and $R_t=r$, i.e the state and reward will be $s’$ and $r$. This probability is determined by the particular values of the preceding state $s \\in \\boldsymbol{S}$ and action $a \\in \\boldsymbol{A}(s)$.Note that $\\boldsymbol{A}(s)$ is the set of actions that can be taken from state $s$\n",
    "\n",
    "Let’s define this probability.\n",
    "\n",
    "For all $s’ \\in \\boldsymbol{S}$, $s \\in \\boldsymbol{S}$, $r \\in \\boldsymbol{R}$ and $a \\in \\boldsymbol{A}(s)$ we define the probability of the transition to state $s’$ with reward $r$ from taking action $a$ in state $s$ as :\n",
    "\n",
    "$\\begin{equation*} p\\left( s^{\\prime },r\\mid s,a\\right) =\\Pr \\left\\{ S_{t}=s^{\\prime },R_{t}=r\\mid S_{t-1}=s,A_{t-1}=a\\right\\} \\text{.} \\end{equation*}$\n",
    "\n",
    "\n",
    "The state and reward at time t depend on which of the following?\n",
    "    - State-action pair for time (t-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Return - What Drives A Reinforcement Learning Agent In An MDP\n",
    "\n",
    "The goal of an agent in an MDP is to **maximize its cumulative rewards**. We need a way to aggregate and formalize these cumulative rewards. For this, we introduce the concept of the expected return of the rewards at a given time step.\n",
    "\n",
    "For now, we can think of the return simply as the sum of future rewards. Mathematically, we define the return $G$ at time $t$ as:\n",
    "\n",
    "$\\begin{equation*} G_{t}=R_{t+1}+R_{t+2}+R_{t+3}+\\cdots +R_{T}\\text{,} \\end{equation*}$\n",
    "\n",
    "where $T$ is the final time step\n",
    "\n",
    "This concept of the expected return is super important because it's the agent's objective to maximize the expected return. The expected return is what's driving the agent to make the decisions it makes.\n",
    "\n",
    "#### Episodic Vs. Continuing Tasks\n",
    "\n",
    "In our definition of the expected return, we introduced $T$, the final time step. When the notion of having a final time step makes sense, the agent-environment interaction naturally breaks up into subsequences, called episodes. For example, think about playing a game of pong. Each new round of the game can be thought of as an episode, and the final time step of an episode occurs when a player scores a point.\n",
    "\n",
    "Thus, each episode ends in a terminal state at time $T$ which is followed by resetting the environment to some standard starting state or to a random sample from a distribution of possible starting states. The next episode then begins independently from how the previous episode ended (again think of Pong).\n",
    "\n",
    "Formally, tasks with episodes are called **episodic tasks**.\n",
    "\n",
    "There exists other types of tasks though where the agent-environment interactions don’t break up naturally into episodes, but instead continue without limit. These types of tasks are called **continuing tasks**.\n",
    "\n",
    "Continuing tasks make our definition of the return at each time $t$ problematic because our final time step would br $T = \\infty$ and therefore the return itself could be infinite since we have\n",
    "\n",
    "$\\begin{equation*} G_{t}=R_{t+1}+R_{t+2}+R_{t+3}+\\cdots +R_{T}\\text{.} \\end{equation*}$\n",
    "\n",
    "Because of this, we need to refine they way we're working with the return.\n",
    "\n",
    "#### Discounted Return\n",
    "\n",
    "Our revision of the way we think about return will make use of discounting. Rather than the agent’s goal being to maximize the expected return of rewards, it will instead be to **maximize the expected discounted return of rewards**. Specifically, the agent will be choosing action $A_t$  at each time $t$ to maximize the expected discounted return.\n",
    "\n",
    "To define the discounted return, we first define the discount rate: $\\gamma$ to be a number bw 0 and 1. The discount rate will be the rate for which we discount future rewards and will determine the present value of future rewards. With this, we define the discounted return as:\n",
    "\n",
    "$\\begin{eqnarray*} G_{t} &=&R_{t+1}+\\gamma R_{t+2}+\\gamma ^{2}R_{t+3}+\\cdots \\\\ &=&\\sum_{k=0}^{\\infty }\\gamma ^{k}R_{t+k+1}\\text{.} \\end{eqnarray*}$\n",
    "\n",
    "This definition of the discounted return makes it to where our agent will care more about the **immediate reward over future rewards since future rewards will be more heavily discounted**. So, while the agent does consider the rewards it expects to receive in the future, the more immediate rewards have more influence when it comes to the agent making a decision about taking a particular action.\n",
    "\n",
    "Now, check out this relationship below showing how returns at successive time steps are related to each other. We’ll make use of this relationship later.\n",
    "\n",
    "$\\begin{eqnarray*} G_{t} &=&R_{t+1}+\\gamma R_{t+2}+\\gamma ^{2}R_{t+3}+\\gamma ^{3}R_{t+4}+\\cdots \\\\ &=&R_{t+1}+\\gamma \\left( R_{t+2}+\\gamma R_{t+3}+\\gamma ^{2}R_{t+4}+\\cdots \\right) \\\\ &=&R_{t+1}+\\gamma G_{t+1} \\end{eqnarray*}$\n",
    "\n",
    "\n",
    "Also, check this out. Even though the return at time $t$ is a sum of an infinite number of terms, the return is actually finite as long as the reward is nonzero and constant, and $\\gamma \\lt 1$\n",
    "\n",
    "For example, if the reward at each time step is a constant and $\\gamma \\lt 1$ then the return is:\n",
    "\n",
    "$\\begin{equation*} G_{t}=\\sum_{k=0}^{\\infty }\\gamma ^{k}=\\frac{1}{1-\\gamma }\\text{.} \\end{equation*}$\n",
    "\n",
    "This infinite sum yields a finite result. If you want to understand this concept more deeply, then research infinite series convergence. For our purposes though, you’re free to just trust the fact that this is true, and understand the infinite sum of discounted returns is finite if the conditions we outlined are met.\n",
    "\n",
    "> The main take-away here is that it is the agent's objective to maximize the expected discounted return of rewards. While the agent does consider all of the expected future rewards when selecting an action, the more immediate rewards influence the agent greater than rewards that are expected to be received further out due to the discount rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policies And Value Functions - Good Actions For A Reinforcement Learning Agent\n",
    "\n",
    "We discussed the general idea of MDPs and how an agent in an environment can perform actions and get a rewarded for those actions.\n",
    "\n",
    "With all the possible actions that an agent may be able to take in all the possible states of an environment, there are a couple of things that we might be interested in understanding.\n",
    "\n",
    "First, we'd probably like to know how likely it is for an agent to take any given action from any given state. In other words, what is the **probability that an agent will select a specific action from a specific state**? This is where the notion of policies come into play, and we'll expand on this in just a moment.\n",
    "\n",
    "Secondly, in addition to understanding the probability of selecting an action, we'd probably also like to know **how good a given action or a given state is for the agent**. In terms of rewards, selecting one action over another in a given state may increase or decrease the agent's rewards, so knowing this in advance will probably help our agent out with deciding which actions to take in which states. This is where value functions become useful, and we'll also expand on this idea in just a bit.\n",
    "\n",
    "![](https://i.ibb.co/XLZ50SG/diag1.png)\n",
    "\n",
    "#### Policies\n",
    "\n",
    "A policy is a function that maps a given state to probabilities of selecting each possible action from that state. We will use the symbol $\\pi$ to denote a policy.\n",
    "\n",
    "When speaking about policies, formally we say that an agent “follows a policy.” For example, if an agent follows policy $\\pi$ at time t, then $\\pi(a|s)$ is the probability that $A_t=a$ if $S_t=s$. This means that, at time t, under policy $\\pi$ he probability of taking action $a$ in state $s$ is $\\pi(a|s)$ \n",
    "\n",
    "Note that, for each state $s \\in \\boldsymbol{S}$ $\\pi$ is a probability distribution over $a \\in \\boldsymbol{A}(s)$\n",
    "\n",
    "\n",
    "#### Value Functions\n",
    "\n",
    "Value functions are functions of states, or of state-action pairs, that estimate either:\n",
    "- how good it is for an agent to be in a given state, \n",
    "- or how good it is for the agent to perform a given action in a given state.\n",
    "\n",
    "This notion of how good a state or state-action pair is is given in terms of expected return. Remember, the rewards an agent expects to receive are dependent on what actions the agent takes in given states. So, value functions are defined with respect to specific ways of acting. Since the way an agent acts is influenced by the policy it's following, then we can see that value functions are defined with respect to policies.\n",
    "\n",
    "`Value functions -> states/state-action pairs -> expected return -> actions -> policy`\n",
    "\n",
    "**State-Value function**\n",
    "\n",
    "The state-value function for a policy $\\pi$, denoted as $v_\\pi$ tells us how good any given state is for an agent following policy $\\pi$. In other words, it gives us the value of a state under $\\pi$\n",
    "\n",
    "Formally, the value of state $s$ under policy $\\pi$ is the expected return from starting from state $s$ at time t and following policy $\\pi$ thereafter. Mathematically we define $v_\\pi(s)$ as:\n",
    "\n",
    "$\\begin{eqnarray*} v_{\\pi }\\left( s\\right) &=&E_{\\pi}\\left[\n",
    "                                                \\rule[-0.05in]{0in}{0.2in}G_{t}\\mid S_{t}=s\\right] \\\\ &=&E_{\\pi }\\left[ \\sum_{k=0}^{\\infty }\\gamma ^{k}R_{t+k+1}\\mid S_{t}=s\\right] \\text{.} \\end{eqnarray*}$\n",
    "\n",
    "Here $E$ is the expected value\n",
    "\n",
    "**Action-Value Function**\n",
    "\n",
    "Similarly, the action-value function for policy $\\pi$ denoted as $q_\\pi$ tells us how good it is for the agent to take any given action from a given state while following policy $\\pi$. In other words, it gives us the value of an action under $\\pi$\n",
    "\n",
    "Formally, the value of action $a$ in state $s$ under policy $\\pi$ is the expected return from starting from state $s$ at time $t$, taking action $a$ and and following policy $\\pi$ thereafter. Mathematically, we define $q_\\pi(s,a)$ as:\n",
    "\n",
    "$\\begin{eqnarray*} q_{\\pi }\\left( s,a\\right) &=&E_{\\pi }\\left[ G_{t}\\mid S_{t}=s,A_{t}=a \\rule[-0.05in]{0in}{0.2in}\\right] \\\\ &=&E_{\\pi }\\left[ \\sum_{k=0}^{\\infty }\\gamma ^{k}R_{t+k+1}\\mid S_{t}=s,A_{t}=a\\right] \\text{.} \\end{eqnarray*}$\n",
    "\n",
    "Conventionally, the action-value function $q_\\pi$ is referred to as the **Q-function**, and the **output from the function for any given state-action pair is called a Q-value. The letter “ Q” is used to represent the quality of taking a given action in a given state**. We’ll be working with Q-value functions a lot going forward\n",
    "\n",
    "> At this point, we now have an idea of the structure of MDPs, all the key components, and how, within an MDP, we can measure how good different states or different state-action pairs are for an agent through the use of value functions. Reinforcement learning algorithms estimate value functions as a way to determine best routes for the agent to take.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python37464bit534221b4ba144d7c9105a79e50b044ad"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
